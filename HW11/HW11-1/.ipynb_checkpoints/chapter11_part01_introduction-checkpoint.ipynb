{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Deep learning for text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Natural-language processing: The bird's eye view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Text standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Text splitting (tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Vocabulary indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using the TextVectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Displaying the vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Two approaches for representing groups of words: Sets and sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the IMDB movie reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0 80.2M    0  160k    0     0  65965      0  0:21:15  0:00:02  0:21:13 66011\n",
      "  1 80.2M    1  896k    0     0   262k      0  0:05:13  0:00:03  0:05:10  262k\n",
      "  2 80.2M    2 2464k    0     0   557k      0  0:02:27  0:00:04  0:02:23  557k\n",
      "  7 80.2M    7 5776k    0     0  1067k      0  0:01:16  0:00:05  0:01:11 1163k\n",
      "  9 80.2M    9 7920k    0     0  1236k      0  0:01:06  0:00:06  0:01:00 1588k\n",
      " 12 80.2M   12  9.8M    0     0  1356k      0  0:01:00  0:00:07  0:00:53 2007k\n",
      " 14 80.2M   14 11.7M    0     0  1435k      0  0:00:57  0:00:08  0:00:49 2236k\n",
      " 16 80.2M   16 13.3M    0     0  1442k      0  0:00:56  0:00:09  0:00:47 2213k\n",
      " 17 80.2M   17 14.3M    0     0  1411k      0  0:00:58  0:00:10  0:00:48 1782k\n",
      " 19 80.2M   19 15.3M    0     0  1374k      0  0:00:59  0:00:11  0:00:48 1551k\n",
      " 20 80.2M   20 16.4M    0     0  1353k      0  0:01:00  0:00:12  0:00:48 1350k\n",
      " 21 80.2M   21 17.5M    0     0  1338k      0  0:01:01  0:00:13  0:00:48 1177k\n",
      " 23 80.2M   23 18.6M    0     0  1324k      0  0:01:02  0:00:14  0:00:48 1094k\n",
      " 24 80.2M   24 19.8M    0     0  1315k      0  0:01:02  0:00:15  0:00:47 1116k\n",
      " 26 80.2M   26 20.9M    0     0  1306k      0  0:01:02  0:00:16  0:00:46 1149k\n",
      " 27 80.2M   27 22.1M    0     0  1301k      0  0:01:03  0:00:17  0:00:46 1170k\n",
      " 29 80.2M   29 23.4M    0     0  1303k      0  0:01:03  0:00:18  0:00:45 1209k\n",
      " 30 80.2M   30 24.8M    0     0  1309k      0  0:01:02  0:00:19  0:00:43 1267k\n",
      " 33 80.2M   33 26.5M    0     0  1330k      0  0:01:01  0:00:20  0:00:41 1378k\n",
      " 35 80.2M   35 28.4M    0     0  1358k      0  0:01:00  0:00:21  0:00:39 1529k\n",
      " 37 80.2M   37 30.4M    0     0  1393k      0  0:00:58  0:00:22  0:00:36 1713k\n",
      " 40 80.2M   40 32.2M    0     0  1407k      0  0:00:58  0:00:23  0:00:35 1787k\n",
      " 41 80.2M   41 33.6M    0     0  1411k      0  0:00:58  0:00:24  0:00:34 1809k\n",
      " 43 80.2M   43 35.2M    0     0  1421k      0  0:00:57  0:00:25  0:00:32 1792k\n",
      " 46 80.2M   46 36.9M    0     0  1430k      0  0:00:57  0:00:26  0:00:31 1740k\n",
      " 47 80.2M   47 38.3M    0     0  1430k      0  0:00:57  0:00:27  0:00:30 1600k\n",
      " 49 80.2M   49 39.4M    0     0  1421k      0  0:00:57  0:00:28  0:00:29 1488k\n",
      " 50 80.2M   50 40.4M    0     0  1409k      0  0:00:58  0:00:29  0:00:29 1398k\n",
      " 51 80.2M   51 41.3M    0     0  1393k      0  0:00:58  0:00:30  0:00:28 1252k\n",
      " 52 80.2M   52 42.2M    0     0  1378k      0  0:00:59  0:00:31  0:00:28 1101k\n",
      " 53 80.2M   53 43.1M    0     0  1364k      0  0:01:00  0:00:32  0:00:28  994k\n",
      " 54 80.2M   54 44.0M    0     0  1349k      0  0:01:00  0:00:33  0:00:27  943k\n",
      " 56 80.2M   56 45.0M    0     0  1339k      0  0:01:01  0:00:34  0:00:27  923k\n",
      " 57 80.2M   57 45.8M    0     0  1327k      0  0:01:01  0:00:35  0:00:26  921k\n",
      " 58 80.2M   58 46.8M    0     0  1318k      0  0:01:02  0:00:36  0:00:26  940k\n",
      " 59 80.2M   59 47.7M    0     0  1306k      0  0:01:02  0:00:37  0:00:25  937k\n",
      " 60 80.2M   60 48.6M    0     0  1295k      0  0:01:03  0:00:38  0:00:25  931k\n",
      " 61 80.2M   61 49.5M    0     0  1286k      0  0:01:03  0:00:39  0:00:24  924k\n",
      " 63 80.2M   63 50.5M    0     0  1281k      0  0:01:04  0:00:40  0:00:24  955k\n",
      " 64 80.2M   64 51.5M    0     0  1273k      0  0:01:04  0:00:41  0:00:23  947k\n",
      " 65 80.2M   65 52.5M    0     0  1269k      0  0:01:04  0:00:42  0:00:22  987k\n",
      " 66 80.2M   66 53.4M    0     0  1258k      0  0:01:05  0:00:43  0:00:22  979k\n",
      " 67 80.2M   67 54.3M    0     0  1253k      0  0:01:05  0:00:44  0:00:21  993k\n",
      " 68 80.2M   68 55.2M    0     0  1244k      0  0:01:06  0:00:45  0:00:21  953k\n",
      " 69 80.2M   69 56.1M    0     0  1238k      0  0:01:06  0:00:46  0:00:20  947k\n",
      " 71 80.2M   71 57.2M    0     0  1235k      0  0:01:06  0:00:47  0:00:19  951k\n",
      " 72 80.2M   72 58.2M    0     0  1232k      0  0:01:06  0:00:48  0:00:18  999k\n",
      " 73 80.2M   73 59.3M    0     0  1229k      0  0:01:06  0:00:49  0:00:17 1020k\n",
      " 75 80.2M   75 60.4M    0     0  1226k      0  0:01:06  0:00:50  0:00:16 1064k\n",
      " 76 80.2M   76 61.4M    0     0  1224k      0  0:01:07  0:00:51  0:00:16 1093k\n",
      " 77 80.2M   77 62.5M    0     0  1222k      0  0:01:07  0:00:52  0:00:15 1096k\n",
      " 79 80.2M   79 63.7M    0     0  1222k      0  0:01:07  0:00:53  0:00:14 1129k\n",
      " 81 80.2M   81 65.2M    0     0  1227k      0  0:01:06  0:00:54  0:00:12 1202k\n",
      " 83 80.2M   83 66.7M    0     0  1232k      0  0:01:06  0:00:55  0:00:11 1288k\n",
      " 84 80.2M   84 67.8M    0     0  1231k      0  0:01:06  0:00:56  0:00:10 1301k\n",
      " 86 80.2M   86 69.4M    0     0  1239k      0  0:01:06  0:00:57  0:00:09 1417k\n",
      " 88 80.2M   88 70.6M    0     0  1238k      0  0:01:06  0:00:58  0:00:08 1409k\n",
      " 89 80.2M   89 71.7M    0     0  1236k      0  0:01:06  0:00:59  0:00:07 1338k\n",
      " 90 80.2M   90 72.2M    0     0  1223k      0  0:01:07  0:01:00  0:00:07 1124k\n",
      " 90 80.2M   90 72.9M    0     0  1215k      0  0:01:07  0:01:01  0:00:06 1037k\n",
      " 91 80.2M   91 73.7M    0     0  1209k      0  0:01:07  0:01:02  0:00:05  863k\n",
      " 92 80.2M   92 74.5M    0     0  1204k      0  0:01:08  0:01:03  0:00:05  802k\n",
      " 93 80.2M   93 75.4M    0     0  1198k      0  0:01:08  0:01:04  0:00:04  747k\n",
      " 95 80.2M   95 76.2M    0     0  1194k      0  0:01:08  0:01:05  0:00:03  840k\n",
      " 96 80.2M   96 77.1M    0     0  1189k      0  0:01:09  0:01:06  0:00:03  875k\n",
      " 97 80.2M   97 78.0M    0     0  1185k      0  0:01:09  0:01:07  0:00:02  895k\n",
      " 98 80.2M   98 79.0M    0     0  1181k      0  0:01:09  0:01:08  0:00:01  898k\n",
      " 99 80.2M   99 79.7M    0     0  1174k      0  0:01:09  0:01:09 --:--:--  870k\n",
      " 99 80.2M   99 80.1M    0     0  1166k      0  0:01:10  0:01:10 --:--:--  796k\n",
      "100 80.2M  100 80.2M    0     0  1164k      0  0:01:10  0:01:10 --:--:--  757k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "code"
   },
   "source": [
    "### REMOVE aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "code"
   },
   "source": [
    "### LOOK AT \"aclImdb/train/pos/4077_10.txt\"\n",
    "-\n",
    "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Displaying the shapes and dtypes of the first batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'It doesn\\'t take long to see why Code Name: Diamond Head didn\\'t make it onto the network schedules. The TV pilot movie doesn\\'t get past the credits before it\\'s obvious just how bad it\\'s going to be. Maybe I missed something, because the plot didn\\'t make a whole lot of sense. Based on what I got out of the muddled mess, a terrorist or thief or something named \"Tree\" (Ian McShane) goes to Hawaii to steal something to do with a secret weapon. The world\\'s dullest secret agent, Johnny Paul (Roy Thinnes), is out to stop him. There might have been more, but trust me \\xc2\\x96 it really doesn\\'t matter anyway.<br /><br />Action movies should have action. Suspenseful moments should have suspense. And dramatic moments should have drama. There\\'s none of that in Code Name: Diamond Head. I\\'ve seen others use the word \"turgid\" to describe this made for TV snoozer \\xc2\\x96 and it\\'s better than any one word description I can come up with. None of the characters is in the least bit exciting or worth caring about. And Roy Thinnes makes for the worst leads imaginable. His charisma is just slightly north of a slug. Ian McShane is easily the best thing the movie has going for it, but unfortunately for everyone else involved, it doesn\\'t appear he was going to be back as a regular cast member. Now if McShane had been cast in the series lead, well then you might have had something.<br /><br />I\\'m quickly discovering that these Gawd awful 70s made-for-TV movies make great Mystery Science Theater 3000 episodes. And that goes double if Quinn Martin was involved. Very funny stuff from Mike and the Bots. So while I may only give the movie a 3/10, I rate Episode #608 a 4/5 on my MST3K rating scale.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Processing words as a set: The bag-of-words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Single words (unigrams) with binary encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preprocessing our datasets with a `TextVectorization` layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Inspecting the output of our binary unigram dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Our model-building utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and testing the binary unigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 59s 89ms/step - loss: 0.3963 - accuracy: 0.8360 - val_loss: 0.2975 - val_accuracy: 0.8840\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 22s 35ms/step - loss: 0.2624 - accuracy: 0.9020 - val_loss: 0.3027 - val_accuracy: 0.8858\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 16s 26ms/step - loss: 0.2351 - accuracy: 0.9167 - val_loss: 0.3190 - val_accuracy: 0.8896\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 9s 13ms/step - loss: 0.2222 - accuracy: 0.9257 - val_loss: 0.3356 - val_accuracy: 0.8864\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 13s 22ms/step - loss: 0.2148 - accuracy: 0.9300 - val_loss: 0.3489 - val_accuracy: 0.8884\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.2009 - accuracy: 0.9352 - val_loss: 0.3659 - val_accuracy: 0.8854\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.2032 - accuracy: 0.9373 - val_loss: 0.3783 - val_accuracy: 0.8894\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.2000 - accuracy: 0.9360 - val_loss: 0.3883 - val_accuracy: 0.8860\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.2085 - accuracy: 0.9381 - val_loss: 0.3944 - val_accuracy: 0.8836\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.2057 - accuracy: 0.9382 - val_loss: 0.4015 - val_accuracy: 0.8850\n",
      "782/782 [==============================] - 206s 250ms/step - loss: 0.2899 - accuracy: 0.8875\n",
      "Test acc: 0.888\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Bigrams with binary encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Configuring the `TextVectorization` layer to return bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and testing the binary bigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 25s 37ms/step - loss: 0.3811 - accuracy: 0.8432 - val_loss: 0.2905 - val_accuracy: 0.8898\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 12s 18ms/step - loss: 0.2517 - accuracy: 0.9122 - val_loss: 0.3050 - val_accuracy: 0.8886\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.2258 - accuracy: 0.9280 - val_loss: 0.3195 - val_accuracy: 0.8914\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2036 - accuracy: 0.9352 - val_loss: 0.3363 - val_accuracy: 0.8904\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.1974 - accuracy: 0.9387 - val_loss: 0.3567 - val_accuracy: 0.8920\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.1894 - accuracy: 0.9421 - val_loss: 0.3766 - val_accuracy: 0.8892\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.1877 - accuracy: 0.9468 - val_loss: 0.3899 - val_accuracy: 0.8914\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.1899 - accuracy: 0.9469 - val_loss: 0.3902 - val_accuracy: 0.8884\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.1930 - accuracy: 0.9464 - val_loss: 0.4058 - val_accuracy: 0.8858\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.1826 - accuracy: 0.9492 - val_loss: 0.4170 - val_accuracy: 0.8874\n",
      "782/782 [==============================] - 130s 159ms/step - loss: 0.2748 - accuracy: 0.8952\n",
      "Test acc: 0.895\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Bigrams with TF-IDF encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Configuring the `TextVectorization` layer to return token counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and testing the TF-IDF bigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 36s 55ms/step - loss: 0.4655 - accuracy: 0.7809 - val_loss: 0.3123 - val_accuracy: 0.8822\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 11s 16ms/step - loss: 0.3465 - accuracy: 0.8469 - val_loss: 0.3126 - val_accuracy: 0.8692\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.3138 - accuracy: 0.8619 - val_loss: 0.3219 - val_accuracy: 0.8804\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2923 - accuracy: 0.8740 - val_loss: 0.3151 - val_accuracy: 0.8724\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.2758 - accuracy: 0.8753 - val_loss: 0.3368 - val_accuracy: 0.8726\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2556 - accuracy: 0.8855 - val_loss: 0.3278 - val_accuracy: 0.8644\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2414 - accuracy: 0.8918 - val_loss: 0.3504 - val_accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2364 - accuracy: 0.8956 - val_loss: 0.3406 - val_accuracy: 0.8754\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2471 - accuracy: 0.8974 - val_loss: 0.3468 - val_accuracy: 0.8660\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2347 - accuracy: 0.8974 - val_loss: 0.3800 - val_accuracy: 0.8636\n",
      "782/782 [==============================] - 100s 122ms/step - loss: 0.3088 - accuracy: 0.8828\n",
      "Test acc: 0.883\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.87 percent positive\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part01_introduction.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
